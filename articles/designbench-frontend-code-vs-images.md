---
title: "結局フロントエンド実装はAIに何を渡せばいいの? DesignBench論文でわかった画像よりコードが強い理由とその実データ"
emoji: "🧠"
type: "tech"
topics: ["ai", "frontend", "react", "benchmark"]
published: true
---

AIにフロントエンドUIを実装させるとき、  
あなたは何を渡しているだろう？──**Figmaの画像**か、それとも**既存のコンポーネントコード**か。  

2025年の論文 [**DesignBench (arXiv:2506.06251)**](https://arxiv.org/abs/2506.06251) は、  
この疑問に対して「コードの方が明確に優れている」と、**実験データで結論を出した**。

---

## 🧩 DesignBenchとは
DesignBenchは、React／Vue／Angularなどの主要フレームワークで動作する  
**実際のコンポーネント単位コード**を対象に、以下3タスクでAIを評価するベンチマーク。

| タスク | 内容 | 目的 |
|:--|:--|:--|
| **Generation** | 指示やデザインから新規UIを生成 | コード生成能力 |
| **Edit** | 既存コードに変更を加える | 局所修正能力 |
| **Repair** | バグを含むコードを修正 | 不具合検出と修復能力 |

入力モードは  
①画像のみ、②コードのみ、③画像＋コード（マルチモーダル）の3種類。  
この組み合わせで、どの情報がAIにとって有効かを定量的に比較している。

---

## 🧮 スコアの比較結果

| タスク | 入力条件 | 主な指標 | 平均スコア | 傾向 |
|:--|:--|:--|:--|:--|
| **生成 (Generation)** | 画像＋コード | CLIP類似度 0.73〜0.83<br>コンパイル成功率 >90% | 見た目再現では画像が有効 |
| **編集 (Edit)** | コードのみ | MLLMスコア 8.40〜8.43 | 最も高精度。差分修正が的確 |
|  | 画像のみ | MLLMスコア 7.37〜7.67 | 修正箇所の誤認が多い |
|  | 画像＋コード | 約8.4 | コードのみと大差なし |
| **修復 (Repair)** | コードのみ | MLLMスコア 約6.7<br>成功率 ≒100% | 安定して修復可能 |
|  | 画像のみ | MLLMスコア 約5台 | 意図解釈のミスが多い |

> **編集・修復タスクでは、コード入力が画像入力を1.0〜1.5ポイント上回り、ビルド成功率・修正精度ともに圧倒的に安定。**  
> 一方、生成タスクでは画像情報がレイアウト再現に貢献した。

---

## 🧠 なぜコードが優れていたのか（論文の考察）

DesignBench論文の “Input Context Analysis” セクションには次の記述がある。

> *“We attribute this to the higher semantic density and structure awareness contained in code inputs, whereas image inputs only reflect the rendered results without explicit logic or hierarchy.”*  
> （コード入力は**意味的密度**と**構造認識**を持ち、  
> 画像はレンダリング結果しか示さず、ロジックや階層構造を欠いている。）

つまり、AIにとってコードは「構造化された説明文」なのに対し、画像は「結果のスクリーンショット」にすぎない。

### 🔍 論文から読み取れる具体的な優位点

| 観点 | コード入力 | 画像入力 |
|:--|:--|:--|
| **構造理解** | DOM階層やコンポーネント境界を直接把握できる | 見た目から推測するしかない |
| **意味的情報** | props・state・関数名などからUIの意図を推定できる | 背景ロジックが欠落 |
| **修正範囲の局所化** | 該当関数やタグ単位で差分を特定できる | 修正箇所を推定する必要がある |
| **検証可能性** | コンパイル・テストで正誤を自動判定できる | 見た目比較しかできない |
| **抽象化・再利用** | コンポーネント化・変数化が容易 | 抽象構造を推測できない |

実際に論文の実験では、画像入力のみのモデルは  
- **修正対象の誤特定**  
- **不要な全体書き換え**  
- **構文エラーの多発**  
といった失敗パターンを頻繁に示した。  
対してコード入力モデルは**局所修正・差分生成・再利用構造保持**に優れた。

---

## 💡 実務へのヒント：「AIに何を渡せばよいか」

| 目的 | AIに渡すべき情報 | 理由 |
|:--|:--|:--|
| **UI生成** | デザイン画像 or Figmaリンク＋フレームワーク指定 | 見た目を再現したい場合は画像も有効 |
| **デザイン調整・リファクタ** | 対象コンポーネントコード＋変更指示 | コードが意図を含むため、修正精度が高い |
| **バグ修正** | 該当コード＋テスト結果 or エラーログ | AIが修復箇所を正確に特定できる |
| **デバッグ** | エラー再現手順＋開発者ツールで取得したDOM断片 | スクリーンショットより構造情報が豊富で、差分を正確に伝えられる |

📌 **ポイント**：
- 「どのファイルのどの部分をどう直すか」を**コードで明示**する。
- スクリーンショットは補助的に使い、**構造情報を主役にする。**
- デバッグ時も画面キャプチャより、開発者ツールからコピーしたDOMを共有する。

もしデザインシステムが整備され、Figma上でコンポーネントが揃っているなら、[figma mcpでコード取得](https://zenn.dev/reiwatravel/articles/13828b9659798a)を利用して、設計情報をそのままコードで渡すのも効果的だ。

---

## 🚀 DesignBenchが教えてくれること

DesignBenchの結果を一言でまとめると──  
> **AIは「見た目」より「構造」で理解する。**

- 生成タスク：画像情報が役立つ  
- 編集・修復タスク：コードが圧倒的に有効  
- マルチモーダル：改善効果は限定的  

---

## 🧭 まとめ
AIフロント実装におけるベストプラクティスはこうだ。

1. 新規UIは**画像＋フレームワーク指定**で渡す  
2. 修正やリファクタは**該当コード抜粋＋変更要件**で渡す  
3. 成果は**ビルド通過率やテスト合格**で自動評価  

DesignBenchのメッセージはシンプルだ。  
> **「AIはあなたの目ではなく、設計を読む」**

だからこそ、次にAIにUIを任せるときは、  
スクリーンショットではなく**コード片**を渡してみよう。  
それが、AIにとって最も理解しやすい「言語」だ。
